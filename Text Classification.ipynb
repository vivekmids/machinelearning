{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will work with text data from newsgroup postings on a variety of topics. We will train classifiers to distinguish between the topics based on the text of the posts. We will represent each document with a \"bag-of-words\" model. As you'll see, this makes the feature representation quite sparse, i.e. only a few words of the total vocabulary are active in any given document. The bag-of-words assumption here is that the label depends only on the words; their order is not important.\n",
    "\n",
    "The SK-learn documentation on feature extraction will prove useful:\n",
    "http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "For those with some experience with NLP know the NTLK library provides a number of useful features. We will not use it here since the idea is to demonstrate the concepts and not necessarily to acheive a high accuracy rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data, stripping out metadata so that we learn classifiers that only use textual features. By default, newsgroups data is split into train and test sets. We further split the test so we have a dev set. \n",
    "\n",
    "We've taken a subset of categories to keep to allow classifiers execute quickly. You can change the categories attribute in fetch_20newsgroups function to get all categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     categories=categories)\n",
    "\n",
    "# setting up the train, test and dev set variables.\n",
    "num_test = len(newsgroups_test.target)\n",
    "num_dev = int(num_test/2)\n",
    "\n",
    "test_data, test_labels = newsgroups_test.data[(num_dev):], newsgroups_test.target[(num_dev):]\n",
    "dev_data, dev_labels = newsgroups_test.data[:num_dev], newsgroups_test.target[:num_dev]\n",
    "train_data, train_labels = newsgroups_train.data, newsgroups_train.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some basic EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking sizes of data sets and label names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training label shape: (3569,)\n",
      "test label shape: (1188,)\n",
      "dev label shape: (1188,)\n",
      "labels names: ['misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt']\n"
     ]
    }
   ],
   "source": [
    "print ('training label shape:', train_labels.shape)\n",
    "print ('test label shape:', test_labels.shape)\n",
    "print ('dev label shape:', dev_labels.shape)\n",
    "print ('labels names:', newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us see what the data looks like For each of the first 5 training examples, print the text of the message along with the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text of type :  rec.autos\n",
      "Text :\n",
      "I'd like to converse with anyone who has purchased a 1993 Honda\n",
      "Civic about their experience.  I'm new to the car buying game\n",
      "and would like to know what price I can expect to pay for a sedan\n",
      "after bargaining.\n",
      "\n",
      "Thanks in advance,\n",
      "------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Text of type :  misc.forsale\n",
      "Text :\n",
      "This drive is less than one year old.  The cartridges have all been bought\n",
      "since then.  All is in excellent condition and still under warranty.\n",
      "Due to a change in system use, I now need a large, contiguous drive.\n",
      "\n",
      "Offer includes:\n",
      "\tSyDos 44i internal drive\n",
      "\tSCSI adapter card and cables\n",
      "\tAll original documentation\n",
      "\tSoftware\n",
      "\tAll original packaging\n",
      "\t8 cartridges totalling over 350Mb (no bad sectors or defects)\n",
      "\n",
      "The installation was a breeze on my 386 clone.\n",
      "\n",
      "I will trade for something near 300Mb IDE, or sell for $450.\n",
      "I will also consider trading for 4 4Mx9 30 pin SIMMs at 70ns.\n",
      "------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Text of type :  rec.autos\n",
      "Text :\n",
      "\n",
      "\n",
      "\n",
      "Vell...Let's see...vas you muzzah in der passenger seat?  Or vas you muzzah in\n",
      "der leefing room, vit you fazah?\n",
      "------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Text of type :  rec.sport.baseball\n",
      "Text :\n",
      "\n",
      "\n",
      "\tStrictly from memory, I think the Phillies were something like\n",
      "\tten games up with 12 to go, lost 10 in a row, and 11 of last 12\n",
      "\tto lose to the Cardinals. Seems impossible, but thats how I\n",
      "\tremember it. I also felt at the time that Johnny Callison of\n",
      "\tthe Phillies lost the MVP as a by-product of their swoon.\n",
      "\n",
      "\n",
      "\t\t\t\t\tjerry\n",
      "------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Text of type :  sci.crypt\n",
      "Text :\n",
      "\n",
      "\n",
      "\n",
      "Whew!  Take it easy on the guy.  Maybe he's going to do this in his\n",
      "spare time.  Maybe he's going to do this to see how much a wiretap\n",
      "*really* costs.  Maybe he's going to do this so he can add to the\n",
      "opposition to Clipper.  I don't know fully why he might do this, but\n",
      "maybe we shouldn't start flaming at the drop of a hat.  \n",
      "------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def examples(num_examples=5):\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        print(\"Text of type : \", newsgroups_train.target_names[train_labels[i]])\n",
    "        print(\"Text :\")\n",
    "        print(train_data[i])\n",
    "        print(\"------------------------------------------------------------------\")\n",
    "        print()\n",
    "        print()\n",
    "          \n",
    "\n",
    "examples(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring CountVectorizer\n",
    "\n",
    "CountVectorizer turns the raw training text into feature vectors. The fit_transform functionmakes 2 passes through the data: first it computes the vocabulary (\"fit\"), second it converts the raw text into feature vectors using the vocabulary (\"transform\").\n",
    "\n",
    "The vectorizer has a lot of options. To get familiar with some of them, we will write code to answer these questions:\n",
    "\n",
    "a. The output of the transform (also of fit_transform) is a sparse matrix: http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html. What is the size of the vocabulary? What is the average number of non-zero features per example? What fraction of the entries in the matrix are non-zero? \n",
    "\n",
    "b. What are the 0th and last feature strings (in alphabetical order)?\n",
    "\n",
    "c. Ability to take custom vocabulary. Specify your own vocabulary with 4 words: [\"atheism\", \"graphics\", \"space\", \"religion\"]. Check the average number of non-zero features per example with custom vocabulary?\n",
    "\n",
    "d. Instead of extracting unigram word features, use \"analyzer\" and \"ngram_range\" to extract bigram and trigram **character** features. Check the size vocabulary now.\n",
    "\n",
    "e. Use the \"min_df\" argument to prune words that appear in fewer than 10 documents. What size vocabulary does this yield?\n",
    "\n",
    "f. Using the standard CountVectorizer, what fraction of the words in the dev data are missing from the vocabulary? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a. Base case\n",
      "-------\n",
      "Size of Vocabulary :  32403\n",
      "Average non zero features per example :  145.91\n",
      "Fraction of non zero entries  0.00259\n",
      "\n",
      "\n",
      "b. feature names\n",
      "-------\n",
      "First few features are '['00', '000', '0000', '00000000', '0000000004']' and last few features are '['zz', 'zzcrm', 'zzi776', 'zzzzzz', 'zzzzzzt']'\n",
      "\n",
      "\n",
      "c. Custom vocabulary\n",
      "-------\n",
      "Size of Vocabulary :  6\n",
      "Average non zero features per example :  0.45\n",
      "Fraction of non zero entries  0.041702\n",
      "\n",
      "\n",
      "d. Character feature with ngram = 2\n",
      "-------\n",
      "Size of Vocabulary :  4410\n",
      "Average non zero features per example :  890.12\n",
      "Fraction of non zero entries  0.045992\n",
      "\n",
      "\n",
      "d. Character feature with ngram = 3\n",
      "-------\n",
      "Size of Vocabulary :  45286\n",
      "Average non zero features per example :  889.15\n",
      "Fraction of non zero entries  0.009426\n",
      "\n",
      "\n",
      "e. Exclude infrequent words\n",
      "-------\n",
      "Size of Vocabulary :  4193\n",
      "Average non zero features per example :  124.17\n",
      "Fraction of non zero entries  0.016003\n",
      "\n",
      "\n",
      "f. Comparing vocabulary of training and dev data sets:\n",
      "-------\n",
      "Training data has 21153 features that are not in Dev\n",
      "Fraction of features in training data missing in dev are:  0.653\n"
     ]
    }
   ],
   "source": [
    "def exploreCountVectorizer():\n",
    "\n",
    "    def printUtil(X):\n",
    "        sizeVocab = X.shape[1]\n",
    "        aveNonZero = np.average(np.sum(X, axis=1))\n",
    "        fracNonZero = X.nnz/(X.shape[0]*X.shape[1])\n",
    "        print(\"-------\")\n",
    "        print(\"Size of Vocabulary : \", sizeVocab)\n",
    "        print(\"Average non zero features per example : \", round(aveNonZero, 2))\n",
    "        print(\"Fraction of non zero entries \", round(fracNonZero, 6))\n",
    "        print()\n",
    "        print()  \n",
    "\n",
    "    \n",
    "### Base case\n",
    "    countVector = CountVectorizer()\n",
    "    X = countVector.fit_transform(train_data)\n",
    "    print(\"a. Base case\")\n",
    "    printUtil(X)\n",
    "\n",
    "### feature names\n",
    "    featureNames = countVector.get_feature_names()\n",
    "    print(\"b. feature names\")\n",
    "    print(\"-------\")\n",
    "    print(\"First few features are '\"+ str(featureNames[:5]) + \"' and last few features are '\" + str(featureNames[-5:]) + \"'\")\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "### custom vocabulary\n",
    "    myVocab = ['sale', 'car', 'motorcycle', 'baseball', 'hockey', 'science']\n",
    "    countVector = CountVectorizer(vocabulary=myVocab)\n",
    "    X = countVector.fit_transform(train_data)\n",
    "    print(\"c. Custom vocabulary\")\n",
    "    printUtil(X)   \n",
    "    \n",
    "### Character features and n-grams\n",
    "    ngram_range = (2,2)\n",
    "    countVector = CountVectorizer(ngram_range = ngram_range, analyzer='char')\n",
    "    X = countVector.fit_transform(train_data)\n",
    "    print(\"d. Character feature with ngram = 2\")\n",
    "    printUtil(X)   \n",
    "\n",
    "    ngram_range = (3,3)\n",
    "    countVector = CountVectorizer(ngram_range = ngram_range, analyzer='char')\n",
    "    X = countVector.fit_transform(train_data)\n",
    "    print(\"d. Character feature with ngram = 3\")\n",
    "    printUtil(X) \n",
    "\n",
    "### exclude infrequent words\n",
    "    countVector = CountVectorizer(min_df=10)\n",
    "    X = countVector.fit_transform(train_data)\n",
    "    print(\"e. Exclude infrequent words\")\n",
    "    printUtil(X)   \n",
    "\n",
    "### vocabulary in training and dev will be different\n",
    "    countVector = CountVectorizer()\n",
    "    countVector.fit_transform(train_data)\n",
    "    trainFeatures = countVector.get_feature_names()\n",
    "    countVector.fit_transform(dev_data)\n",
    "    devFeatures = countVector.get_feature_names()\n",
    "    missingInDev = len(set(trainFeatures).difference(set(devFeatures)))\n",
    "    \n",
    "    print(\"f. Comparing vocabulary of training and dev data sets:\")\n",
    "    print(\"-------\")\n",
    "    print(\"Training data has\", missingInDev, \"features that are not in Dev\")\n",
    "    print(\"Fraction of features in training data missing in dev are: \", round(missingInDev/len(trainFeatures), 3))\n",
    "\n",
    "\n",
    "exploreCountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Classifiers\n",
    "\n",
    "Let us start with:\n",
    "\n",
    "* KNN\n",
    "* Naive Bayes\n",
    "* Logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors :\n",
      "Best K Nearest Neighbors fit is found when n_neighbors = 5\n",
      "F1 with dev data of this model is  0.3754\n",
      "\n",
      "Multinominal Naive Bayes :\n",
      "Best Multinominal Naive Bayes fit is found when alpha = 0.014\n",
      "F1 with dev data of this model is  0.8266\n",
      "\n",
      "Logistic Regression :\n",
      "Best Logistic Regression fit is found when C = 0.1\n",
      "F1 with dev data of this model is  0.7668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def basicClassifiers():\n",
    "\n",
    "\n",
    "    countVector = CountVectorizer()\n",
    "    X = countVector.fit_transform(train_data)\n",
    "    countVector1 = CountVectorizer(vocabulary=countVector.get_feature_names())\n",
    "    XDev = countVector1.fit_transform(dev_data)\n",
    "    \n",
    "    def modelPerformance(classifier, model):\n",
    "        y_predict = classifier.predict(XDev)\n",
    "        f1=metrics.f1_score(dev_labels, y_predict, average='micro')\n",
    "        key = [*classifier.best_params_][0]\n",
    "        print(model, ':')\n",
    "        print('Best', model, 'fit is found when',key, '=', classifier.best_params_.get(key))\n",
    "        print('F1 with dev data of this model is ', round(f1, 4))\n",
    "        print()\n",
    "    \n",
    "# Nearest Neighbors\n",
    "    classifier = KNeighborsClassifier()\n",
    "    ks = {'n_neighbors':[1,2,3,4,5,6,7,8,9,10]}\n",
    "    grid = GridSearchCV(classifier, ks, cv=5, scoring='f1_micro', n_jobs=-1)\n",
    "    trained_classifier = grid.fit(X, train_labels)\n",
    "    \n",
    "    modelPerformance(trained_classifier, 'K Nearest Neighbors')\n",
    "    \n",
    "# Naive Bayes\n",
    "    classifier = MultinomialNB()\n",
    "    alphas = {'alpha': [ 0.01, .011, .012, .013, .014, .015, .016, .017, .018, .019, .1]}\n",
    "    grid = GridSearchCV(classifier, alphas, cv=5, scoring='f1_micro', n_jobs=-1)\n",
    "    trained_classifier = grid.fit(X, train_labels)\n",
    "\n",
    "    modelPerformance(trained_classifier, 'Multinominal Naive Bayes')\n",
    "    \n",
    "# Logistic\n",
    "    classifier = LogisticRegression(random_state=0, solver='newton-cg', multi_class='multinomial')\n",
    "    Cs = {'C': [0.01, 0.1, .25, 0.5, .75]}\n",
    "    grid = GridSearchCV(classifier, Cs, cv=5, scoring='f1_micro', n_jobs=-1)\n",
    "    trained_classifier = grid.fit(X, train_labels)\n",
    "\n",
    "    modelPerformance(trained_classifier, 'Logistic Regression')\n",
    "\n",
    "basicClassifiers()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaways**\n",
    "\n",
    "1. Nearest neighbors performs poorly as compared to other models. Consider a model with a single word vocabulary and the trained model expecting 2 occurrences for a certain class. For the model, test samples containing zero or four occurrences are equally far. However, we intuitively know that the one with 4 occurrences is more likely to be close to the class compared to the text with zero occurrences. In fact sample texts with more than 4 occurrences will actually be further away from trained model as compared to text with zero occurrences.\n",
    "\n",
    "2. Naive Bayes vs Logistic regression: In this case Naive Bayes performs better than logistic regression. However, logistic regression is almost always better than a generative model such as Naive Bayes when large amount of data is available. So, with smaller data sets a generative model might give a good performance but as more and more training data is available, Logistic regression should take over. More on this topic in paper from Andrew Ng and Michael Jordan (https://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 regularization \n",
    "Logistic regression estimates a weight vector for each class, which can be accessed with the coef\\_ attribute. Recall that L2 regularization attempts to minimize the sum of square coefficients. Here we will see how the sum of square coefficients vary with strength of regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demonstration of l2 regularization\n",
      "[[  0.01    2.515   2.39    2.476   2.45    2.554   2.314]\n",
      " [  0.1    18.118  20.909  22.577  21.154  19.813  17.24 ]\n",
      " [  0.25   35.365  42.818  47.391  43.632  39.79   33.646]\n",
      " [  0.5    55.556  69.182  78.381  71.216  64.119  52.604]\n",
      " [  0.75   70.745  89.431 102.648  92.662  82.976  66.664]]\n"
     ]
    }
   ],
   "source": [
    "def l2Regularization():\n",
    "    \n",
    "    countVector = CountVectorizer()\n",
    "    X = countVector.fit_transform(train_data)\n",
    "    countVector1 = CountVectorizer(vocabulary=countVector.get_feature_names())\n",
    "    XDev = countVector1.fit_transform(dev_data)\n",
    "    \n",
    "    listC = [0.01, 0.1, .25, 0.5, .75]\n",
    "    table = np.empty((0,0))\n",
    "    for C in listC:\n",
    "        classifier = LogisticRegression(random_state=0, solver='newton-cg', multi_class='multinomial', C=C)\n",
    "        trained_classifier = classifier.fit(X, train_labels)\n",
    "        table = np.append(table, C)\n",
    "        for i in trained_classifier.classes_:\n",
    "            sumSqWt = np.matmul(trained_classifier.coef_[i], np.transpose(trained_classifier.coef_[i]))\n",
    "            table = np.append(table, round(sumSqWt, 3))\n",
    "    np.set_printoptions(suppress=True)\n",
    "    print('Demonstration of l2 regularization')\n",
    "    print(table.reshape((len(listC),-1)))\n",
    "\n",
    "l2Regularization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaways**   \n",
    "This demonstrates effect of regularization on size of wieghts. L2 regularization measures the size of weights as sum of squared weights. Higher the C, lower the regularization and lower the regularization higher the sum of squared weights. This is what we see in the table above. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A closer look at the features\n",
    "\n",
    "Here we will 5 features with the largest weights for each label. We will create a table with 20 rows and 4 columns that shows the weight for each of these features for each of the labels. Create another table with bigram features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "#  Feature         sale   auto  bike  baseball hockey science\n",
      "1  price         [ 0.586  0.269 -0.269 -0.254 -0.099 -0.234]\n",
      "2  sell          [ 0.775 -0.17  -0.141 -0.187 -0.075 -0.202]\n",
      "3  sale          [ 0.877 -0.188  0.01  -0.276 -0.208 -0.215]\n",
      "4  offer         [ 0.9   -0.064 -0.343 -0.266 -0.201 -0.026]\n",
      "5  shipping      [ 0.968 -0.255 -0.186 -0.205 -0.217 -0.105]\n",
      "6  gt            [-0.022  0.559 -0.166 -0.157 -0.125 -0.09 ]\n",
      "7  dealer        [-0.171  0.572 -0.109 -0.109 -0.1   -0.083]\n",
      "8  auto          [-0.021  0.612 -0.203 -0.151 -0.117 -0.12 ]\n",
      "9  cars          [-0.134  0.945 -0.172 -0.283 -0.092 -0.264]\n",
      "10 car           [-0.177  1.377 -0.076 -0.476 -0.236 -0.413]\n",
      "11 ride          [-0.039 -0.278  0.693 -0.148 -0.109 -0.12 ]\n",
      "12 bikes         [-0.129 -0.288  0.812 -0.113 -0.117 -0.165]\n",
      "13 motorcycle    [-0.124 -0.297  0.851 -0.141 -0.127 -0.163]\n",
      "14 dod           [-0.159 -0.319  1.109 -0.233 -0.201 -0.197]\n",
      "15 bike          [-0.135 -0.653  1.503 -0.24  -0.225 -0.251]\n",
      "16 braves        [-0.039 -0.091 -0.113  0.644 -0.326 -0.076]\n",
      "17 runs          [-0.092 -0.127 -0.046  0.697 -0.283 -0.149]\n",
      "18 cubs          [-0.124 -0.193 -0.183  0.724 -0.125 -0.099]\n",
      "19 jewish        [-0.093 -0.154 -0.183  0.742 -0.2   -0.112]\n",
      "20 baseball      [-0.106 -0.31  -0.316  1.01   0.017 -0.294]\n",
      "21 playoffs      [-0.086 -0.096 -0.105 -0.294  0.648 -0.068]\n",
      "22 mask          [-0.104 -0.154 -0.195 -0.174  0.691 -0.063]\n",
      "23 team          [-0.336 -0.38  -0.395  0.603  0.766 -0.259]\n",
      "24 nhl           [-0.144 -0.104 -0.127 -0.386  0.864 -0.102]\n",
      "25 hockey        [-0.18  -0.246 -0.234 -0.432  1.262 -0.17 ]\n",
      "26 government    [-0.017 -0.141 -0.197 -0.103 -0.162  0.62 ]\n",
      "27 key           [-0.113 -0.071 -0.34  -0.046 -0.157  0.727]\n",
      "28 keys          [-0.098 -0.208 -0.191 -0.14  -0.115  0.753]\n",
      "29 clipper       [-0.098 -0.154 -0.193 -0.234 -0.175  0.853]\n",
      "30 encryption    [-0.144 -0.247 -0.228 -0.162 -0.145  0.926]\n",
      "\n",
      "\n",
      "----------------------------\n",
      "#  Feature         sale   auto  bike  baseball hockey science\n",
      "1  if interested [ 0.582 -0.129 -0.139 -0.124 -0.1   -0.09 ]\n",
      "2  best offer    [ 0.63  -0.062 -0.112 -0.171 -0.157 -0.129]\n",
      "3  looking for   [ 0.638 -0.078 -0.067 -0.207 -0.165 -0.122]\n",
      "4  brand new     [ 0.773 -0.155 -0.121 -0.195 -0.16  -0.142]\n",
      "5  for sale      [ 1.859 -0.407 -0.169 -0.517 -0.405 -0.361]\n",
      "6  what do       [-0.079  0.361 -0.028 -0.116  0.002 -0.139]\n",
      "7  the new       [-0.118  0.38  -0.223 -0.049  0.025 -0.016]\n",
      "8  about the     [-0.242  0.421 -0.036 -0.136 -0.105  0.097]\n",
      "9  the dealer    [-0.086  0.517 -0.143 -0.094 -0.093 -0.1  ]\n",
      "10 the car       [-0.085  0.964 -0.063 -0.284 -0.249 -0.282]\n",
      "11 shaft drive   [-0.064 -0.093  0.357 -0.068 -0.059 -0.072]\n",
      "12 the road      [-0.116  0.133  0.362 -0.026 -0.174 -0.179]\n",
      "13 my bike       [-0.047 -0.136  0.392 -0.074 -0.07  -0.065]\n",
      "14 on my         [-0.063 -0.02   0.555 -0.215 -0.101 -0.157]\n",
      "15 the bike      [-0.137 -0.348  1.025 -0.183 -0.174 -0.183]\n",
      "16 last year     [-0.069 -0.15  -0.159  0.462  0.121 -0.204]\n",
      "17 the mets      [-0.054 -0.111 -0.103  0.474 -0.123 -0.084]\n",
      "18 the cubs      [-0.097 -0.155 -0.146  0.601 -0.095 -0.108]\n",
      "19 the braves    [-0.069 -0.127 -0.129  0.634 -0.194 -0.115]\n",
      "20 this year     [-0.143 -0.275 -0.214  0.659  0.174 -0.202]\n",
      "21 the ice       [-0.053 -0.13  -0.128 -0.144  0.533 -0.077]\n",
      "22 the pens      [-0.065 -0.123 -0.111 -0.173  0.556 -0.084]\n",
      "23 he was        [-0.15  -0.234 -0.227  0.301  0.561 -0.251]\n",
      "24 the playoffs  [-0.061 -0.146 -0.12  -0.208  0.641 -0.106]\n",
      "25 the nhl       [-0.102 -0.18  -0.152 -0.282  0.846 -0.129]\n",
      "26 regards vesselin [-0.061 -0.116 -0.103 -0.1   -0.083  0.463]\n",
      "27 the key       [-0.072 -0.075 -0.126 -0.125 -0.088  0.486]\n",
      "28 the clipper   [-0.075 -0.152 -0.136 -0.145 -0.132  0.64 ]\n",
      "29 the nsa       [-0.074 -0.171 -0.148 -0.133 -0.139  0.664]\n",
      "30 the government [-0.053 -0.202 -0.2   -0.136 -0.189  0.779]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def features():\n",
    "\n",
    "    def myFeatures(ngram_range, analyzer = 'word', numTopFeatures = 5):\n",
    "        countVector = CountVectorizer(ngram_range=ngram_range, analyzer = analyzer)\n",
    "        X = countVector.fit_transform(train_data)\n",
    "\n",
    "        classifier = LogisticRegression(random_state=0, solver='newton-cg', multi_class='multinomial', C=0.25)\n",
    "        trainedClassifier = classifier.fit(X, train_labels)\n",
    "\n",
    "        topFeatureIndicies = np.argpartition(trainedClassifier.coef_, range(-numTopFeatures,0), axis=1)[0:6, -numTopFeatures:]\n",
    "        topFeatureIndicies = topFeatureIndicies.flatten()\n",
    "        featureNames = [countVector.get_feature_names()[i] for i in topFeatureIndicies]\n",
    "        weights = np.transpose(np.round(trainedClassifier.coef_[:, topFeatureIndicies], 3))\n",
    "\n",
    "        print(\"----------------------------\")\n",
    "        print(\"#  Feature         sale   auto  bike  baseball hockey science\")\n",
    "        for s, n, w in zip(range(1,31), featureNames, weights):\n",
    "            print(str(s).ljust(2), n.ljust(13), w)\n",
    "        print()\n",
    "        print()\n",
    "    \n",
    "    myFeatures((1,1))\n",
    "    myFeatures((2,2))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaways**\n",
    "\n",
    "Most single word features seem to be related to the corresponding labels however some associations like 'jewish' related to baseball and 'government' related to science are not intuitive.\n",
    "\n",
    "For bigram case, top 5 features are quite surprising:\n",
    "1. 'regards vesselin' is among the top features for sci.crypt. Probably Vesselin is an avid contributor to this category of posts. While this is interesting, this may not generalize well to unseen data, especially from forums where Vesselin isn't registered.\n",
    "2. Combination of typical stop words such as 'he was', 'the new' and 'what do' turn out to be top features. These are unlikely to generatize well. As we will see shortly, that it is a good practice to remove stop words before running classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data\n",
    "\n",
    "Now we will try a few preprocessing techniques to improve the logistic regression classifier. The preprocessing function runs on the raw text, before it is split into words by the tokenizer. \n",
    "\n",
    "Some common techniques are: \n",
    "\n",
    "* lowercasing everything \n",
    "* replacing sequences of numbers with a single token, \n",
    "* removing various other non-letter characters\n",
    "* shortening long words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 metric is : 0.8165\n"
     ]
    }
   ],
   "source": [
    "def empty_preprocessor(s):\n",
    "    return s\n",
    "\n",
    "def betterPreprocessor(s):\n",
    "\n",
    "    newS = s\n",
    "    \n",
    "    #convert to lowercase\n",
    "    newS = newS.lower() \n",
    "    \n",
    "    # remove common words\n",
    "    #list of stopwords from NTLK\n",
    "    stopWords = ['i','me','my','myself','we','our','ours','ourselves','you','your','yours','yourself',\n",
    "                 'yourselves','he','him','his','himself','she','her','hers','herself','it','its','itself',\n",
    "                 'they','them','their','theirs','themselves','what','which','who','whom','this','that','these',\n",
    "                 'those','am','is','are','was','were','be','been','being','have','has','had','having','do',\n",
    "                 'does','did','doing','a','an','the','and','but','if','or','because','as','until','while','of',\n",
    "                 'at','by','for','with','about','against','between','into','through','during','before','after',\n",
    "                 'above','below','to','from','up','down','in','out','on','off','over','under','again','further',\n",
    "                 'then','once','here','there','when','where','why','how','all','any','both','each','few','more',\n",
    "                 'most','other','some','such','no','nor','not','only','own','same','so','than','too','very','s',\n",
    "                 't','can','will','just','don','should','now']\n",
    "\n",
    "    \n",
    "    for word in stopWords:\n",
    "        newS = re.sub(r'\\b'+word+r'\\b', '', newS)\n",
    "    \n",
    "#simple stemming\n",
    "    newS = re.sub(r's\\b', '', newS)\n",
    "    newS = re.sub(r'es\\b', '', newS)\n",
    "    newS = re.sub(r'ies\\b', '', newS)\n",
    "\n",
    "#replace numbers by string 'num'\n",
    "    newS = re.sub('\\d+','num', newS) \n",
    "\n",
    "#remove special characters\n",
    "    newS = re.sub('(\\\\W|\\\\d)',' ',newS)\n",
    "\n",
    "    return newS\n",
    "\n",
    "\n",
    "def preprocess():\n",
    "\n",
    "    def runLR(preproc = empty_preprocessor):\n",
    "        countVector = CountVectorizer(preprocessor=preproc)\n",
    "        X = countVector.fit_transform(train_data)\n",
    "        XDev = countVector.transform(dev_data)\n",
    "\n",
    "        classifier = LogisticRegression(random_state=0, solver='newton-cg', multi_class='multinomial')\n",
    "        classifier.fit(X, train_labels)\n",
    "        y_predict = classifier.predict(XDev)\n",
    "        f1=metrics.f1_score(dev_labels, y_predict, average='micro')\n",
    "        print('F1 metric is :', round(f1, 4))\n",
    "    \n",
    "    runLR(betterPreprocessor)\n",
    "    \n",
    "\n",
    "preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Takeaways**\n",
    "\n",
    "Better preprocesser did not improve the performance in this example, however these techniques are known to improve generalization to unseen data. Reader is encouraged to tweak the preprocessing steps and see how the model performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding most effective features\n",
    "\n",
    "The idea of regularization is to avoid learning very large weights (which are likely to fit the training data, but not generalize well) by adding a penalty to the total size of the learned weights. That is, logistic regression seeks the set of weights that minimizes errors in the training data AND has a small size. The default regularization, L2, computes this size as the sum of the squared weights (see P3, above). L1 regularization computes this size as the sum of the absolute values of the weights. The result is that whereas L2 regularization makes all the weights relatively small, L1 regularization drives lots of the weights to 0, effectively removing unimportant features.\n",
    "\n",
    "To find the most effective features, we will train a logistic regression model using a \"l1\" penalty. Output the number of learned weights that are not equal to zero. Then we will reduce the size of the vocabulary by keeping only those features that have at least one non-zero weight and retrain a model using \"l2\".\n",
    "\n",
    "We will draw a plot showing accuracy of the re-trained model vs. the vocabulary size you get when pruning unused features by adjusting the C parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When C = 1, L1 regularization has 34593 non zero weights. While L2 regularization has 194418 non zero weights.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2299cb070b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def mostEffectiveFeatures():\n",
    "    # Keep this random seed here to make comparison easier.\n",
    "    np.random.seed(0)\n",
    "\n",
    "    \n",
    "    countVector = CountVectorizer()\n",
    "    X = countVector.fit_transform(train_data)\n",
    "    features = countVector.get_feature_names()\n",
    "    numFeatures = len(features)\n",
    "\n",
    "    def runLR(data, regularization = \"l2\", c=1):\n",
    "        if regularization == \"l1\":\n",
    "            classifier = LogisticRegression(random_state=0, solver='saga', multi_class='multinomial', penalty='l1', tol=.01, C=c)\n",
    "        else:\n",
    "            classifier = LogisticRegression(random_state=0, solver='newton-cg', multi_class='multinomial', C=c)\n",
    "\n",
    "        classifier.fit(data, train_labels)\n",
    "        return classifier\n",
    "\n",
    "    def getNonZeroWeights(classifier):\n",
    "        weights = classifier.coef_\n",
    "        return np.sum(weights!=0)\n",
    "    \n",
    "    def getAccuracy(c):\n",
    "        l1Classifier = runLR(X, 'l1', c)\n",
    "        vocab = [features[x] for x in range(numFeatures) if sum(l1Classifier.coef_[:,x]) !=0]\n",
    "        l2CountVector = CountVectorizer(vocabulary=vocab)\n",
    "        trainData = l2CountVector.fit_transform(train_data)\n",
    "        devData = l2CountVector.transform(dev_data)\n",
    "        \n",
    "        l2Classifier = runLR(trainData, 'l2', c)\n",
    "        yPredict= l2Classifier.predict(devData)\n",
    "        accuracy = metrics.accuracy_score(dev_labels, yPredict)\n",
    "        return accuracy, len(vocab)\n",
    "    \n",
    "    l1classifier = runLR(X, 'l1')\n",
    "    l2classifier = runLR(X, 'l2')\n",
    "    \n",
    "#compare non zero weights between l1 vs l2\n",
    "    nonZeroL1 = getNonZeroWeights(l1classifier)\n",
    "    nonZeroL2 = getNonZeroWeights(l2classifier)\n",
    "    print('When C = 1, L1 regularization has', nonZeroL1, 'non zero weights. While L2 regularization has', nonZeroL2, 'non zero weights.')\n",
    "    \n",
    "#getting accuracies for different Cs\n",
    "\n",
    "    Cs = [.01, .1, .25, .5, .75, 1, 5, 10, 100]\n",
    "    acc = np.empty((0))\n",
    "    lenVocabByC = np.empty((0))\n",
    "    for c in Cs:\n",
    "        accuracy, lenVocab = getAccuracy(c)\n",
    "        acc = np.append(acc, accuracy)\n",
    "        lenVocabByC = np.append(lenVocabByC, lenVocab)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    ax=plt.subplot2grid((1,2), (0,0))\n",
    "    ax.plot(np.log(Cs), acc)\n",
    "    ax.set_xlabel(\"Log of C\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_title(\"Accuracy by Log C\")\n",
    "\n",
    "    ax=plt.subplot2grid((1,2), (0,1))\n",
    "    ax.plot(np.log(Cs), lenVocabByC)\n",
    "    ax.set_xlabel(\"Log of C\")\n",
    "    ax.set_ylabel(\"Size of the vocabulary\")\n",
    "    ax.set_title(\"Size of Vocabulary by Log C\")\n",
    "    \n",
    "\n",
    "    \n",
    "mostEffectiveFeatures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TfidfVectorizer\n",
    "\n",
    "CountVectorizer provides a count of number of times a word appears in a given input. On the other hand, TfidfVectorizer counts the occurences of each word in the input as well but it normalizes the count based on how frequent the word is across samples. This reduces weight on common words that appear very frequently across samples and are not very helpful in classification.\n",
    "\n",
    "In addition to using Tdidf, we will see some of the mistakes that the model makes. For this, we will find documents where the ratio R is largest, where R is maximum predicted probability / predicted probability of the correct label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Label : rec.sport.baseball\n",
      "Predicted Label: rec.sport.hockey\n",
      "Text :\n",
      "\n",
      "\n",
      "In fact, he's a complete and total dickhead on at least 2 newsgroups\n",
      "(this one and rec.sport.hockey).  Since hockey season is almost over,\n",
      "he's back to being a dickhead in r.s.bb.\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Correct Label : rec.autos\n",
      "Predicted Label: rec.motorcycles\n",
      "Text :\n",
      "best way to reduce risk when operating a vehicle is being able to avoid\n",
      "hazards and, for that reason my preferred vehicle is a motorcycle.  When I do\n",
      "use a four wheeler my primary reasons are: it will keep me dry, it will keep\n",
      "me warm, or it will carry more cargo.  If the four wheeler has as much\n",
      "collision protection as the average motorcycle, then it has enough form me.\n",
      "\n",
      "How do you define safe?  One definition of safe is without risk.  Is\n",
      "\n",
      "\n",
      "-- \n",
      "Chas                         DoD #7769\n",
      "------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Correct Label : rec.autos\n",
      "Predicted Label: misc.forsale\n",
      "Text :\n",
      "I'm not sure if this made it out so i'll try again.\n",
      "\n",
      "I have an Ecklar's (sp?) Corvette car cover for sale.  The cover is canvas\n",
      "on the outside and felt on the inside.  It is weather proof and in great\n",
      "condition.  I'm asking $95.00 and I'll pay shipping.  (originally $175.00\n",
      "in October of 1992).\n",
      "------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def mistakes():\n",
    "    \n",
    "    def printSamples(indexes):\n",
    "        for i in indexes:\n",
    "            print(\"Correct Label :\", newsgroups_train.target_names[dev_labels[i]])\n",
    "            print(\"Predicted Label:\", newsgroups_train.target_names[y_predict[i]])\n",
    "            print(\"Text :\")\n",
    "            print(dev_data[i])\n",
    "            print(\"------------------------------------------------------------------\")\n",
    "            print()\n",
    "            print()\n",
    "        \n",
    "    countVector = TfidfVectorizer()\n",
    "    X = countVector.fit_transform(train_data)\n",
    "    XDev = countVector.transform(dev_data)\n",
    "\n",
    "    classifier = LogisticRegression(random_state=0, solver='newton-cg', multi_class='multinomial', C=100)\n",
    "    classifier.fit(X, train_labels)\n",
    "    \n",
    "    y_predict = classifier.predict(XDev)\n",
    "    \n",
    "    pp = classifier.predict_proba(XDev)\n",
    "    mpp = np.max(pp, axis=1)\n",
    "    \n",
    "    i=0\n",
    "    ppocl = np.empty((0))\n",
    "    for j in dev_labels:\n",
    "        ppocl = np.append(ppocl, pp[i,j])\n",
    "        i+=1\n",
    "\n",
    "    R = np.divide(mpp, ppocl)\n",
    "    \n",
    "    top3 = np.argpartition(R, range(-3,0))[-3:]\n",
    "    \n",
    "    printSamples(top3)\n",
    "    \n",
    "\n",
    "mistakes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaways**\n",
    "\n",
    "Looking at mistakes in the classification, seems like confusion is inherent in the text. For example, if someone is comparing motorbikes and autos, it may be tricky to correctly predict which of the 2 groups did the writer submitted their post to. This limited is expected in a 'bag of words' model which predicts based on words and ignores their order.\n",
    "\n",
    "**Ideas to improve performance**  \n",
    "\n",
    "* Include sequence of words instead of just a single word. In fact, given top features seen in P4, the top features don't seem all that relevant and hence maybe the classification should be done entirely based on combination of words.\n",
    "* Preprocess data similar to P5.\n",
    "* Choose a appropriate regularization to improve how the model generalizes to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "Picking the most promisng options we have seen so far to see how good a classifer we can build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2) :\n",
      "Best fit is found when C = 1\n",
      "F1 with dev data of this model is  0.8283\n",
      "\n",
      "(1, 3) :\n",
      "Best fit is found when C = 1\n",
      "F1 with dev data of this model is  0.8333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def betterPreprocessor(s):\n",
    "\n",
    "    newS = s\n",
    "    \n",
    "    #convert to lowercase\n",
    "    newS = newS.lower() \n",
    "    \n",
    "    # remove common words\n",
    "    #list of stopwords from NTLK\n",
    "    stopWords = ['i','me','my','myself','we','our','ours','ourselves','you','your','yours','yourself',\n",
    "                 'yourselves','he','him','his','himself','she','her','hers','herself','it','its','itself',\n",
    "                 'they','them','their','theirs','themselves','what','which','who','whom','this','that','these',\n",
    "                 'those','am','is','are','was','were','be','been','being','have','has','had','having','do',\n",
    "                 'does','did','doing','a','an','the','and','but','if','or','because','as','until','while','of',\n",
    "                 'at','by','for','with','about','against','between','into','through','during','before','after',\n",
    "                 'above','below','to','from','up','down','in','out','on','off','over','under','again','further',\n",
    "                 'then','once','here','there','when','where','why','how','all','any','both','each','few','more',\n",
    "                 'most','other','some','such','no','nor','not','only','own','same','so','than','too','very','s',\n",
    "                 't','can','will','just','don','should','now']\n",
    "\n",
    "    \n",
    "    for word in stopWords:\n",
    "        newS = re.sub(r'\\b'+word+r'\\b', '', newS)\n",
    "    \n",
    "#simple stemming\n",
    "    newS = re.sub(r's\\b', '', newS)\n",
    "    newS = re.sub(r'es\\b', '', newS)\n",
    "    newS = re.sub(r'ies\\b', '', newS)\n",
    "\n",
    "#replace numbers by string 'num'\n",
    "    newS = re.sub('\\d+','num', newS) \n",
    "\n",
    "#replace numbers by string 'num'\n",
    "    newS = re.sub('\\d+','num', newS) \n",
    "\n",
    "#remove special characters\n",
    "    newS = re.sub('(\\\\W|\\\\d)',' ',newS)\n",
    "\n",
    "    return newS\n",
    "\n",
    "\n",
    "def allTogether():\n",
    "    \n",
    "    def runLR(ngram_range, analyzer = 'word'):\n",
    "        countVector = TfidfVectorizer(ngram_range=ngram_range, analyzer = analyzer, preprocessor=betterPreprocessor, min_df=10)\n",
    "        X = countVector.fit_transform(train_data)\n",
    "        XDev = countVector.transform(dev_data) \n",
    "\n",
    "        Cs = {'C': [1, 10, 100]}\n",
    "        classifier = LogisticRegression(random_state=0, solver='newton-cg', multi_class='multinomial')\n",
    "        grid = GridSearchCV(classifier, Cs, cv=5, scoring='f1_micro', n_jobs=-1)\n",
    "        trained_classifier = grid.fit(X, train_labels)\n",
    "        y_predict = grid.predict(XDev)\n",
    "        f1=metrics.f1_score(dev_labels, y_predict, average='micro')\n",
    "        key = [*grid.best_params_][0]\n",
    "        print(ngram_range, ':')\n",
    "        print('Best fit is found when',key, '=', grid.best_params_.get(key))\n",
    "        print('F1 with dev data of this model is ', round(f1, 4))\n",
    "        print()\n",
    "    \n",
    "    ngrams = [(1,2), (1,3)]\n",
    "    for ngram in ngrams:\n",
    "        runLR(ngram)\n",
    "        \n",
    "allTogether()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially by tweaking various handles that we explored in this assignment, some of which are shown in the code above, we are able to improve the performance upto 83. using combination of :  \n",
    "1. Data preprocessing to reduce the noise\n",
    "2. Tfidf Vector instead of simple count vector\n",
    "3. 1, 2 and 3 word vocabulary instead of single word vocabulary\n",
    "4. An optimal regularization parameter\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
